{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 3**\n",
    "\n",
    "Due: **October 8th, 5pm** (late submission until October 11th, 5pm -- no submission possible afterwards)\n",
    "\n",
    "Written assignment: 10 points\n",
    "\n",
    "Coding assignment: 25 points\n",
    "\n",
    "Project report: 15 points\n",
    "\n",
    "### Name: Qiuli Lai\n",
    "\n",
    "### Link to the github repo: https://github.com/098pipi/data2060_hw3.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written assignment\n",
    "\n",
    "### **Gradient Descent (10 points)**\n",
    "\n",
    "Consider using gradient descent to find the minimum of $f$, where,\n",
    "- $f$ is a convex function over the closed interval \\[-*b*,*b*\\], *b* > 0\n",
    "- $f'$ is the derivative of $f$\n",
    "- $\\alpha$ is some positive number which will represent a learning rate parameter\n",
    "\n",
    "The steps of gradient descent are as follows:\n",
    "\n",
    "- Start at $x_{0} = 0$\n",
    "- At each step, set $x_{t+1} = x_{t} - \\alpha f'(x_{t})$\n",
    "- If $x_{t+1}$ falls below  -*b*, set it to -*b*, and if it goes above *b*, set it to *b*.\n",
    "\n",
    "We say that an optimization algorithm (such as gradient descent)\n",
    "*$\\epsilon$-converges* if, at some point, $x_{t}$ stays within  $\\epsilon$ of\n",
    "the true minimum. Formally, we have *$\\epsilon$-convergence* at time $t$ if\n",
    "\n",
    "$\\quad \\quad |x_{t'}-x_{\\min}| \\le \\epsilon, \\quad \\text{where } x_{\\min}=\\underset{x \\in [-b,b]}{argmin} f(x)$ for all $t' \\geq t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 1**\n",
    "For $\\alpha$ = 0.1, *b* = 1, and $\\epsilon$ = 0.001, find a convex function $f$ so that running gradient descent does not  $\\epsilon$-converge.\n",
    "Specifically, make it so that *x*<sub>0</sub> = 0,\n",
    "$x_1$ = *b*,  $x_2$ =  - *b*,  $x_3$ = *b*,  $x_4$ =  - *b*, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "Parameters:  \n",
    "$\\alpha = 0.1,\\; b = 1,\\; \\epsilon = 0.001$  \n",
    "\n",
    "**Iteration steps:**  \n",
    "$$\n",
    "x_0 = 0 \\\\\n",
    "x_1 = x_0 - 0.1 f'(x_0) \\\\\n",
    "1 = 0 - 0.1 f'(0) \\\\\n",
    "f'(0) = -10\n",
    "$$\n",
    "\n",
    "$$\n",
    "-1 = 1 - 0.1 f'(1) \\\\\n",
    "f'(1) = 20\n",
    "$$\n",
    "\n",
    "$$\n",
    "1 = -1 - 0.1 f'(-1) \\\\\n",
    "f'(-1) = -20\n",
    "$$\n",
    "\n",
    "**Form of derivative:**  \n",
    "$$\n",
    "f'(x) = ax^2 + bx + c\n",
    "$$\n",
    "\n",
    "Using conditions:  \n",
    "$$\n",
    "f'(0) = c = -10 \\\\\n",
    "f'(1) = a + b - 10 = 20 \\\\\n",
    "f'(-1) = a - b - 10 = -20\n",
    "$$\n",
    "\n",
    "Solve system:  \n",
    "$$\n",
    "a = 10,\\; b = 20\n",
    "$$\n",
    "\n",
    "So,  \n",
    "$$\n",
    "f'(x) = 10x^2 + 20x - 10 + K\n",
    "$$\n",
    " since K is irrelevant for gradient descent, we set K = 0\n",
    "\n",
    "Integrating:  \n",
    "$$\n",
    "f(x) = \\tfrac{10}{3}x^3 + 10x^2 - 10x\n",
    "$$\n",
    "\n",
    "**Convexity on [-1,1]**\n",
    "\n",
    "f''(x) = 20x + 20 on [-1,1], f''(x) in a monotonically increasing function, min{f''(x)} = f''(-1) = 0, hence f''(x) >= 0 for any x in [-1, 1], f is convex on the closed interval [-1, 1]\n",
    "\n",
    "**Convergence check:**  \n",
    "$$\n",
    "x_{\\min} = 0, \\quad |x_t - x_{\\min}| = 1 > \\epsilon = 0.001 \\;\\;\\; \\forall \\; t > 0\n",
    "$$\n",
    "\n",
    "Thus, the method diverges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "For *$\\alpha$* = 0.1, *b* = 1, and  $\\epsilon$ = 0.001, find a convex function $f$\n",
    "so that gradient descent does *$\\epsilon$-converge*, but only after at least\n",
    "10,000 steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "For simplicity, consider f a quadratic function\n",
    "$$f(x) = \\tfrac{\\mu}{2}x^2, \\quad \\mu > 0 \\\\\n",
    "f'(x) = \\mu x. \\\\\n",
    "f''(x) = \\mu > 0\n",
    "$$\n",
    "so f(x) is convex\n",
    "\n",
    "$$x_{n+1} = x_t - 0.1f'(x_n) = (1 - 0.1\\mu)\\,x_n.$$\n",
    "\n",
    "Define the convergence rate \n",
    "    $$r := 1 - 0.1 \\mu$$\n",
    "\n",
    "By induction,\n",
    "$$x_n = r^n x_0$$\n",
    "\n",
    "Choose that \n",
    "$$x_n = \\epsilon, x_0 = 1, n = 10^4$$\n",
    "$$r^n = \\frac{\\varepsilon}{\\lvert x_0 \\rvert}.$$\n",
    "\n",
    "$$r = \\left(\\frac{\\varepsilon}{\\lvert x_0 \\rvert}\\right)^{1/n}, \\quad \n",
    "\\mu = \\frac{1-r}{0.1}.$$\n",
    "\n",
    "$$r = 10^{-3/10000}, \\quad \\mu \\approx 0.00690537.$$\n",
    "\n",
    "$$f(x) = 0.00345268 \\, x^2.$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Coding Assignment (25 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evironment test below, make sure you get all green checks. If not, you will lose 2 points for each red or missing sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.13.5\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.10.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.7.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pytest version 8.4.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m torch version 2.7.1 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.11\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.11\"):\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.10.5\", 'numpy': \"2.3.2\",'sklearn': \"1.7.1\", \n",
    "                'pandas': \"2.3.2\", 'pytest': \"8.4.1\", 'torch':\"2.7.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this assignment, you will be using a modified version of the UCI\n",
    "Census Income data set to predict the education levels of individuals\n",
    "based on certain attributes collected from the 1994 census database. You\n",
    "can read more about the dataset here:\n",
    "[`https://archive.ics.uci.edu/ml/datasets/Census+Income`](https://archive.ics.uci.edu/ml/datasets/Census+Income).  \n",
    "\n",
    "### Stencil Code\n",
    "\n",
    "We have provided the following stencil code within this file:\n",
    "\n",
    "-   `Model` contains the `LogisticRegression` model you will be\n",
    "    implementing.\n",
    "\n",
    "-   `Check Model` contains a series of tests to ensure you are coding your \n",
    "    model properly.\n",
    "\n",
    "-   `Main` is the entry point of program which will read in the\n",
    "    dataset, run the model, and print the results.\n",
    "\n",
    "You should not modify any code in `Check Model` and `Main`. If you do for debugging\n",
    "or other purposes, please make sure any additions are commented out in\n",
    "the final handin. All the functions you need to fill in reside in this notebook,\n",
    "marked by `TODO`s. You can see a full description of them in the section\n",
    "below.\n",
    "\n",
    "### **The Assignment**\n",
    "\n",
    "In `Model`, there are a few functions you will implement. They are:\n",
    "\n",
    "-   `LogisticRegression`:\n",
    "\n",
    "    -   **train()** uses stochastic gradient descent to train the\n",
    "        weights of the model.\n",
    "\n",
    "    -   **loss()** calculates the log loss of some dataset divided by\n",
    "        the number of examples.\n",
    "\n",
    "    -   **predict()** predicts the labels of data points using the\n",
    "        trained weights. For each data point, you should apply the\n",
    "        softmax function to it and return the label with the highest\n",
    "        assigned probability.\n",
    "\n",
    "    -   **accuracy()** computes the percentage of the correctly\n",
    "        predicted labels over a dataset.\n",
    "\n",
    "*Note*: You are not allowed to use any packages that have already\n",
    "implemented these models (e.g. scikit-learn). We have also included some\n",
    "code in `main` for you to test out the different random seeds and\n",
    "calculate the average accuracy of your model across those random seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic Regression**\n",
    "\n",
    "Logistic Regression, despite its name, is used in classification\n",
    "problems. It learns sigmoid functions of the inputs\n",
    "$$h_{\\bf w}(x)_j = \\phi_{sig}(\\langle {\\bf w}_j, {\\bf x} \\rangle)$$\n",
    "where $h_{\\bf w}(x)_j$ is the probability that sample\n",
    "$\\bf x$ is a member of class *j*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-class classification, we need to apply the `softmax` function\n",
    "to normalize the probabilities of each class. The loss function of a\n",
    "Logistic Regression classifier over *k* classes on a *single* example\n",
    "(*x*,*y*) is the **log-loss**, sometimes called **cross-entropy loss**:\n",
    "$$\\ell(h_{\\bf w}, ({\\bf x}, y)) = - \\sum_{j = 1}^{k}\n",
    "\\left\\{\\begin{array}{lr}\n",
    "    \\log( h_{\\bf w}({\\bf x})_j ), & y = j\\\\\n",
    "    0, & \\text{otherwise} \\\\\n",
    "\\end{array}\\right\\}$$\n",
    "Therefore, the ERM hypothesis of **w** on a dataset of *m* samples has weights\n",
    "$$\n",
    "{\\bf w} = \\underset{\\bf w}{argmin} (-\\frac{1}{m}\\sum_{i = 1}^m \\sum_{j = 1}^{k}\n",
    "\\left\\{\\begin{array}{lr}\n",
    "    \\log( h_{\\bf w}({\\bf x}_i)_j), & y_{i} = j\\\\\n",
    "    0, & \\text{otherwise} \\\\\n",
    "\\end{array}\\right\\} )\n",
    "$$\n",
    "To learn the ERM hypothesis, we need to perform gradient descent. The\n",
    "partial derivative of the loss function on a single data point\n",
    "$$\n",
    "\\frac{\\partial l_S(h_{\\bf w})}{\\partial {\\bf w}_{st}} =\n",
    "\\left\\{\\begin{array}{lr}\n",
    "    h_{\\bf w}({\\bf x})_s - 1, & y = s\\\\\n",
    "    h_{\\bf w}({\\bf x})_s, & \\text{otherwise} \\\\\n",
    "    \\end{array}\\right\\}\n",
    "    {\\bf x}_t\n",
    "$$\n",
    "With respect to a single row in the weights matrix, ${\\bf w}_s$,\n",
    "the partial derivative of the loss is\n",
    "$$\n",
    "\\frac{\\partial l_S(h_{\\bf w})}{\\partial {\\bf w}_{s}} =\n",
    "\\left\\{\\begin{array}{lr}\n",
    "    h_{\\bf w}({\\bf x})_s - 1, & y = s\\\\\n",
    "    h_{\\bf w}({\\bf x})_s, & \\text{otherwise} \\\\\n",
    "    \\end{array}\\right\\}\n",
    "    {\\bf x}\n",
    "$$\n",
    "You will need to descend this gradient to update the weights of your\n",
    "Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stochastic Gradient Descent**\n",
    "\n",
    "You will be using Stochastic Gradient Descent (SGD) to train your\n",
    "`LogisticRegression` model. Below, we have provided pseudocode for SGD\n",
    "on a sample *S*:\n",
    "\n",
    "$\\text{initialize parameters } {\\bf w}\\text{, learning rate } \\alpha \\text{, and batch size b}$  <br />\n",
    "$\\quad \\text{converge = False}$ <br />\n",
    "$\\quad \\text{while not converge:}$ <br />\n",
    "$\\quad \\quad\t\\text{epoch + 1}$ <br />\n",
    "$\\quad \\quad\t\\text{shuffle training examples}$ <br />\n",
    "$\\quad \\quad\t\\text{calculate last epoch loss}$ <br />\n",
    "$\\quad \\quad\t\\text{for } i = 0,1,...,\\lceil{n_{examples}/b}\\rceil-1: \\text{-- iterate over batches:}$ <br />\n",
    "$\\quad \\quad \\quad X_{batch} = X[i \\cdot b: (i+1) \\cdot b] \\text{ -- select the X in the current batch}$ <br />\n",
    "$\\quad \\quad \\quad {\\bf y}_{batch} = {\\bf y}[i \\cdot b: (i+1) \\cdot b] \\text{ -- select the labels in the current batch}$ <br />\n",
    "$\\quad \\quad \\quad \\text{initialize } \\nabla L_{{\\bf w}} \\text{ to be a matrix of zeros}$ <br />\n",
    "$\\quad \\quad \\quad \\text{for each pair of training data point } ({\\bf x},y)\\in (X_{batch}, {\\bf y}_{batch}):$ <br />\n",
    "$\\quad \\quad \\quad \\quad \\text{for }j = 0,1,..., n_{classes} - 1:$ <br />\n",
    "$\\quad \\quad \\quad \\quad \\quad \\text{-- calculate the partial derivative of the loss with respect to}$ <br />\n",
    "$\\quad \\quad \\quad \\quad \\quad \\text{-- a single row in the weights matrix}$ <br />\n",
    "$\\quad \\quad \\quad \\quad \\quad \\text{if }y = j: \\nabla L_{{\\bf w}_j} \\text{ += } \n",
    "(softmax(\\langle {\\bf w}_j, {\\bf x} \\rangle) - 1) \\cdot {\\bf x} $ <br />\n",
    "$\\quad \\quad \\quad \\quad \\quad \\text{else: }\\nabla L_{{\\bf w}_j} \\text{ += } (softmax(\\langle {\\bf w}_j, {\\bf x} \\rangle) ) \\cdot {\\bf x}$ <br />\n",
    "$\\quad \\quad \\quad {\\bf w} = {\\bf w} - \\frac{\\alpha \\nabla L_{\\bf w}}{len(X_{batch})} \\text{ -- update the weights}$ <br />\n",
    "$\\quad \\quad \\text{calculate this epoch loss}$ <br />\n",
    "$\\quad \\quad \\text{if |Loss}(X,{\\bf y})_{this-epoch}-Loss(X,{\\bf y})_{last-epoch}| <  \\text{CONV-THRESHOLD: }$ <br />\n",
    "$\\quad \\quad \\quad \\text{converge = True -- break the loop if loss converged}$\n",
    "\n",
    "\n",
    " **Hints**: Consistent with the notation in the lecture, ${\\bf w}$ are\n",
    "initialized as a *k* x *d* matrix, where *k* is the number of classes\n",
    "and *d* is the number of features (with the bias term). With *n* as the\n",
    "number of examples, *X* is a *n* x *d* matrix, and ${\\bf y}$ is a vector\n",
    "of length *n*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tuning Parameters**\n",
    "\n",
    "Convergence is achieved when the change in loss between iterations is\n",
    "some small value. Usually, this value will be very close to but not\n",
    "equal to zero, so it is up to you to tune this threshold value to best\n",
    "optimize your model's performance. Typically, this number will be some\n",
    "magnitude of 10<sup>-x</sup>, where you experiment with *x*. Note that\n",
    "when calculating the loss for checking convergence, you should be\n",
    "calculating the loss for the entire dataset, not for a single batch\n",
    "(i.e., at the end of every epoch).  \n",
    "  \n",
    "You will also be tuning batch size (and one of the report questions\n",
    "addresses the impact of batch size on model performance). In order to\n",
    "reach the accuracy threshold, you will need to tune both parameters. *$\\alpha$*\n",
    "would typically be tuned during the training process, but we are fixing\n",
    "*$\\alpha$* = 0.03 for this assignment. **Please do not change *$\\alpha$* in your\n",
    "code**.  \n",
    "  \n",
    "You can tune the batch size and convergence threshold in `Main`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extra: Numpy Shortcuts**\n",
    "\n",
    "While optional, there are many numpy shortcuts and functions that can make your code cleaner. We encourage you to look up numpy documentation and learn new functions.\n",
    "\n",
    "Some useful shortcuts:\n",
    "* `A @ B` is a shortcut for `np.matmul(A, B)`\n",
    "* `X.T` is a shortcut for `np.transpose(X)`\n",
    "* `X.shape` is a shortcut for `np.shape(X)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    '''\n",
    "    Apply softmax to an array\n",
    "    @params:\n",
    "        x: the original 1D array\n",
    "    @return:\n",
    "        an 1D array with softmax applied elementwise.\n",
    "    '''\n",
    "    e = np.exp(x - np.max(x))   # avoid overflow of exp()\n",
    "    return (e + 1e-6) / (np.sum(e) + 1e-6)  # avoid division by 0/log(0)\n",
    "\n",
    "class LogisticRegression:\n",
    "    '''\n",
    "    Multiclass Logistic Regression that learns weights using \n",
    "    stochastic gradient descent.\n",
    "    '''\n",
    "    def __init__(self, n_features, n_classes, batch_size, conv_threshold):\n",
    "        '''\n",
    "        Initializes a LogisticRegression classifer.\n",
    "        @attrs:\n",
    "            n_features: the number of features in the classification problem\n",
    "            n_classes: the number of classes in the classification problem\n",
    "            weights: The weights of the Logistic Regression model\n",
    "            alpha: The learning rate used in stochastic gradient descent\n",
    "        '''\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.weights = np.zeros((n_classes, n_features + 1))  # An extra row added for the bias\n",
    "        self.alpha = 0.03  # DO NOT TUNE THIS PARAMETER\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_threshold = conv_threshold\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Trains the model using stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            num_epochs: integer representing the number of epochs taken to reach convergence\n",
    "        '''\n",
    "        # [TODO]\n",
    "        converge = False\n",
    "        num_epochs = 0\n",
    "        # guarantee the diff is huge enough for the first step to diverge\n",
    "        last_loss = float('inf')    \n",
    "        while not converge:\n",
    "            num_epochs += 1\n",
    "            # shuffle training examples, randomly permuting\n",
    "            indices = np.arange(len(X))\n",
    "            np.random.shuffle(indices)\n",
    "            X, Y = X[indices], Y[indices]\n",
    "\n",
    "            for i in range(math.ceil(len(X)/self.batch_size)):\n",
    "                X_batch = X[i*self.batch_size : (i+1)*self.batch_size]\n",
    "                Y_batch = Y[i*self.batch_size : (i+1)*self.batch_size]\n",
    "                gLw = np.zeros((self.n_classes, self.n_features + 1))\n",
    "\n",
    "                for x_i, y_i in zip(X_batch, Y_batch):\n",
    "                    for j in range(self.n_classes):\n",
    "                        # calculate the partial derivative of the loss w.r.t a single row in W\n",
    "                        z = self.weights @ x_i.T    # (k, d) * (d,1) --> (k,1)\n",
    "                        prob = softmax(z.reshape(1,-1).flatten())  # --> (1,k)\n",
    "                        if y_i == j:\n",
    "                            gLw[j] += (prob[j] - 1) * x_i\n",
    "                        else: \n",
    "                            gLw[j] += prob[j] * x_i\n",
    "                # update the weights\n",
    "                self.weights -= (self.alpha * gLw)/len(X_batch)\n",
    "            # calculate the epoch loss\n",
    "            this_loss = self.loss(X, Y)\n",
    "            if abs(this_loss - last_loss) < self.conv_threshold:\n",
    "                converge = True\n",
    "            last_loss = this_loss\n",
    "        return num_epochs\n",
    "    \n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the total log loss on some dataset (X, Y), divided by the number of examples.\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            A float number which is the average loss of the model on the dataset\n",
    "        '''\n",
    "        z = X @ self.weights.T # get(n,k) \n",
    "        prob = np.array([softmax(z_i) for z_i in z])    # apply row_wise\n",
    "        total_prob = prob[np.arange(len(Y)), Y]\n",
    "        total_logLoss = - np.sum(np.log(total_prob))\n",
    "        return total_logLoss/len(X)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned weigths and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        z = X @ self.weights.T  # get(n,k) \n",
    "        prob = np.array([softmax(z_i) for z_i in z]) \n",
    "        predicted_class = np.argmax(prob, axis=1)\n",
    "        return predicted_class\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Outputs the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "        predicted_class = self.predict(X)\n",
    "        correct = np.sum(predicted_class == Y)\n",
    "        return correct/len(Y)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "# Sets random seed for testing purposes\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Creates Test Model with 2 predictors, 2 classes, a Batch Size of 5 and a Threshold of 1e-2\n",
    "test_model1 = LogisticRegression(2, 2, 5, 1e-2)\n",
    "\n",
    "# Creates Test Data\n",
    "x_bias = np.array([[0,4,1], [0,3,1], [5,0,1], [4,1,1], [0,5,1]])\n",
    "y = np.array([0,0,1,1,0])\n",
    "x_bias_test = np.array([[0,0,1], [-5,3,1], [9,0,1], [1,0,1], [6,-7,1]])\n",
    "y_test = np.array([0,0,1,0,1])\n",
    "\n",
    "# Creates Test Model with 2 predictors, 1 classes, a Batch Size of 1 and a Threshold of 1e-2\n",
    "test_model2 = LogisticRegression(2, 3, 1, 1e-2)\n",
    "\n",
    "# Creates Test Data\n",
    "x_bias2 = np.array([[0,0,1], [0,3,1], [4,0,1], [6,1,1], [0,1,1], [0,4,1]])\n",
    "y2 = np.array([0,1,2,2,0,1])\n",
    "x_bias_test2 = np.array([[0,0,1], [-5,3,1], [9,0,1], [1,0,1]])\n",
    "y_test2 = np.array([0,1,2,0])\n",
    "\n",
    "\n",
    "# Test Model Loss\n",
    "assert test_model1.loss(x_bias, y) == pytest.approx(0.693, .001) # Checks if answer is within .001\n",
    "assert test_model2.loss(x_bias2, y2) == pytest.approx(1.099, .001) # Checks if answer is within .001\n",
    "\n",
    "# Test Train Model and Checks Model Weights\n",
    "assert test_model1.train(x_bias, y) == 14\n",
    "assert test_model1.weights == pytest.approx(np.array([[-0.218, 0.231, 0.0174], [ 0.218, -0.231, -0.0174]]), 0.01) # Answer within .01\n",
    "\n",
    "assert test_model2.train(x_bias, y) == 9\n",
    "assert test_model2.weights == pytest.approx(np.array([[-0.300,  0.560,  0.093], [ 0.523, -0.257,  0.032], [-0.226, -0.304, -0.123]]), .05) \n",
    "\n",
    "# Test Model Predict\n",
    "assert (test_model1.predict(x_bias_test) == np.array([0., 0., 1., 1., 1.])).all()\n",
    "assert (test_model2.predict(x_bias_test2) == np.array([0, 0, 1, 1])).all()\n",
    "\n",
    "# Test Model Accuracy\n",
    "assert test_model1.accuracy(x_bias_test, y_test) == .8â€“\n",
    "assert test_model2.accuracy(x_bias_test2, y_test2) == .25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.4%\n",
      "Number of Epochs: 103\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_FILE_NAME = 'normalized_data.csv'\n",
    "# DATA_FILE_NAME = 'unnormalized_data.csv'\n",
    "# DATA_FILE_NAME = 'normalized_data_nosens.csv'\n",
    "\n",
    "CENSUS_FILE_PATH = DATA_FILE_NAME\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 2  # [TODO]: tune this parameter\n",
    "CONV_THRESHOLD = 1e-4 # [TODO]: tune this parameter\n",
    "\n",
    "def import_census(file_path):\n",
    "    '''\n",
    "        Helper function to import the census dataset\n",
    "        @param:\n",
    "            train_path: path to census train data + labels\n",
    "            test_path: path to census test data + labels\n",
    "        @return:\n",
    "            X_train: training data inputs\n",
    "            Y_train: training data labels\n",
    "            X_test: testing data inputs\n",
    "            Y_test: testing data labels\n",
    "    '''\n",
    "    data = np.genfromtxt(file_path, delimiter=',', skip_header=False)\n",
    "    X = data[:, :-1]\n",
    "    Y = data[:, -1].astype(int)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def test_logreg():\n",
    "    X_train, Y_train, X_test, Y_test = import_census(CENSUS_FILE_PATH)\n",
    "    num_features = X_train.shape[1]\n",
    "\n",
    "    # Add a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    ### Logistic Regression ###\n",
    "    model = LogisticRegression(num_features, NUM_CLASSES, BATCH_SIZE, CONV_THRESHOLD)\n",
    "    num_epochs = model.train(X_train_b, Y_train)\n",
    "    acc = model.accuracy(X_test_b, Y_test) * 100\n",
    "    print(\"Test Accuracy: {:.1f}%\".format(acc))\n",
    "    print(\"Number of Epochs: \" + str(num_epochs))\n",
    "\n",
    "# Set random seeds. DO NOT CHANGE THIS IN YOUR FINAL SUBMISSION.\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "test_logreg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model (Cont'd)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy: 77.8%\n",
      "Average Number of Epochs: 2.2\n"
     ]
    }
   ],
   "source": [
    "### test your model on the census dataset\n",
    "X_train, Y_train, X_test, Y_test = import_census(CENSUS_FILE_PATH)\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# Add a bias\n",
    "X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "# Logistic Regression, average accross 10 random states\n",
    "random.seed(0)\n",
    "num_states = 10\n",
    "num_epochs, test_accuracies = [], []\n",
    "\n",
    "for _ in range(num_states):\n",
    "    random_state = random.randint(1, 1000)\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    model = LogisticRegression(num_features, n_classes=3, batch_size=1, conv_threshold=0.1)\n",
    "    num_epochs.append(model.train(X_train_b, Y_train))\n",
    "    test_accuracies.append(model.accuracy(X_test_b, Y_test) * 100)\n",
    "\n",
    "avg_test_accuracy = sum(test_accuracies) / num_states\n",
    "avg_num_epochs = sum(num_epochs) / num_states\n",
    "print(\"Average Test Accuracy: {:.1f}%\".format(avg_test_accuracy))\n",
    "print(\"Average Number of Epochs: \" + str(avg_num_epochs))\n",
    "\n",
    "assert 1.5 < avg_num_epochs < 2.5\n",
    "assert 75 < avg_test_accuracy < 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Report Questions (15 points)**\n",
    "\n",
    "### **Question 1**\n",
    "\n",
    "Make sure that you have implemented a variable batch size using the\n",
    "constructor given for `LogisticRegression`. Try different batch\n",
    "sizes ([1, 8, 64, 512, 4096] - there are ~5700 points in the dataset), and try different convergence thresholds ([1e-1, 1e-2, 1e-3]) in the cell below. Visualize the accuracy and number of epochs taken to converge.\n",
    "\n",
    "Answer the following questions:\n",
    "-   What tradeoffs exist between good accuracy and quick\n",
    "    convergence?\n",
    "-    Why do you think the batch size led to the results you received?\n",
    "\n",
    "Fill in the `generate_array()` and `generate_heatmap()` functions so you can visualize how accuracy and number of epochs taken changes as we change batch size and convergence threshold. Fill out BATCH_SIZE_ARR and CONV_THRESHOLD_ARR with the values described above.\n",
    "\n",
    "-   **generate_array()** should loop through both BATCH_SIZE_ARR and CONV_THRESHOLD_ARR to populate `epoch_arr` and `acc_arr`. Make sure to round `acc_arr` to 2 decimal places before returning (Hint: `np.round`).\n",
    "        \n",
    "-   **generate_heatmap()** should create a matplotlib heatmap of the arrays. You should label the axis and title of each plot using BATCH_SIZE_ARR and CONV_THRESHOLD_ARR. It might be helpful to look at Matplotlib's guide for heatmaps: https://matplotlib.org/stable/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
    "\n",
    "**Hint:** Runs with large batch sizes and low convergence thresholds might take several minutes to half an hour to complete. We recommend that you develop the code below with a small subset of the parameters (e.g., batch size of [1,2,4] and conv_threshold of [1e-1, 1e-2]). Once your code works and your figures look good, rerun everything with the batch size and conv_threshold values described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "BATCH_SIZE_ARR = [1, 8, 64, 512, 4096] # [TODO]: try different values\n",
    "CONV_THRESHOLD_ARR = [1e-1, 1e-2, 1e-3]  # [TODO]: try different values\n",
    "\n",
    "def generate_array():\n",
    "    '''\n",
    "        Runs the logistic regression model on different batch sizes and\n",
    "        convergence thresholds to populate arrays for accuracy and number of epochs taken.\n",
    "        @return:\n",
    "            epoch_arr: 2D array of epochs taken, for each batch size and conv threshold\n",
    "            acc_arr: 2D array of accuracies, for each batch size and conv threshold\n",
    "    '''\n",
    "    X_train, Y_train, X_test, Y_test = import_census(CENSUS_FILE_PATH)\n",
    "    num_features = X_train.shape[1]\n",
    "\n",
    "    # Add a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    # Initializes the accuracy and epoch arrays\n",
    "    acc_arr = np.zeros((len(BATCH_SIZE_ARR), len(CONV_THRESHOLD_ARR)))\n",
    "    epoch_arr = np.zeros((len(BATCH_SIZE_ARR), len(CONV_THRESHOLD_ARR)))\n",
    "\n",
    "    ### Populate arrays ###\n",
    "    for b in range(len(BATCH_SIZE_ARR)):\n",
    "        for c in range(len(CONV_THRESHOLD_ARR)):\n",
    "            model = LogisticRegression(n_features = num_features, \n",
    "                                       n_classes = 3, \n",
    "                                       batch_size = BATCH_SIZE_ARR[b],\n",
    "                                       conv_threshold = CONV_THRESHOLD_ARR[c])\n",
    "            epoch_arr[b][c] = model.train(X = X_train_b, Y = Y_train)\n",
    "            acc_arr[b][c] = np.round(model.accuracy(X = X_test_b, Y = Y_test),2)\n",
    "    return epoch_arr, acc_arr\n",
    "\n",
    "\n",
    "def generate_heatmap(arr, name):\n",
    "    '''\n",
    "        Generates a matplotlib heatmap for an array\n",
    "        convergence thresholds to populate arrays for accuracy and number of epochs taken.\n",
    "        @param:\n",
    "            arr: 2D array to generate heatmap of\n",
    "            name: title of the plot (Hint: use plt.title)\n",
    "        @return:\n",
    "            None\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=(5,3))\n",
    "    im = ax.imshow(arr, cmap = \"viridis\")\n",
    "\n",
    "    ax.set_xticks(np.arange(len(CONV_THRESHOLD_ARR)))\n",
    "    ax.set_yticks(np.arange(len(BATCH_SIZE_ARR)))\n",
    "\n",
    "    ax.set_xticklabels([f'{c:.0e}' for c in CONV_THRESHOLD_ARR])\n",
    "    ax.set_yticklabels(BATCH_SIZE_ARR)\n",
    "\n",
    "    ax.set_xlabel('Convergence Threshold')\n",
    "    ax.set_ylabel('Batch Size')\n",
    "    ax.set_title(name)\n",
    "\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "epoch_arr, acc_arr = generate_array()\n",
    "generate_heatmap(epoch_arr, \"Epochs\")\n",
    "generate_heatmap(acc_arr, \"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Q1: What tradeoffs exist between good accuracy and quick\n",
    "    convergence?\n",
    "\n",
    "A smaller convergence threshold (e.g., 1e-3) forces the model to train for more epochs before stopping, which results in more stable weight updates and higher accuracy. In contrast, a larger threshold (e.g., 1e-1) allows the model to stop earlier, saving computation time but often at the cost of underfitting and lower accuracy. Batch size also plays a critical role: smaller batches lead to noisier gradients and slower convergence, but they sometimes encourage better generalization. Larger batches, on the other hand, provide smoother gradient estimates and converge in fewer epochs, but they can plateau at lower accuracy. Thus, achieving higher accuracy typically requires smaller thresholds and moderate batch sizes, whereas quick convergence is achieved with larger thresholds and larger batches, though with weaker performance.\n",
    "\n",
    "Q2: Why do you think the batch size led to the results you received?\n",
    "\n",
    "From the heatmap, we can see that very small batch sizes (like 1 or 8) lead to higher accuracy, but they also take more epochs because the updates are noisy and slower to settle down. As the batch size grows (64, 512), training becomes faster and smoother, but accuracy starts to drop compared to the small batches. For the very large batch size (4096), the model converges very quickly, but accuracy is the worst, since the updates are too â€œsmoothâ€ and the model stops learning useful patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "Try to run the model with `unnormalized_data.csv` instead of\n",
    "`normalized_data.csv`. Report your findings when running the model\n",
    "on the unnormalized data. In a few short sentences, explain what\n",
    "normalizing the data does and why it affected your model's\n",
    "performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran the model with unnormalized data, the runtime was shorter (fewer epochs), but the accuracy was much lower compared to the normalized dataset. This happens because without normalization, features on very different scales distort gradient updates, leading the model to converge poorly. Normalization balances the feature scales, allowing gradient descent to train more effectively and achieve much higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Solution:**  \n",
    "I found out that the runtime is small(small spochs), however the accuracy is largely low compared to the origianl dataset. In the normalized data, continuous features like \"age,\" \"hours-per-week,\" \"capital-gain\"/\"capital-loss,\"...have been scaled down. The reason is that large-valued features (like capital-gain) produce much larger gradients compared to small features (like education-num), so the optimization heavily skews towards them. This leads to poor convergence and low accuracy. By normalizing the feature, all features are scaled to a similar range, so the model can learn weights fairly across all inputs, converging more stably and accurately. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3**\n",
    "Try the model with `normalized_data_nosens.csv`; in this data file,\n",
    "we have removed the `race` and `sex` attributes. Report your\n",
    "findings on the accuracy of your model on this dataset (averaging\n",
    "over many random seeds here may be useful). Can we make any\n",
    "conclusion based on these accuracy results about whether there is a\n",
    "correlation between sex/race and education level? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** \n",
    "\n",
    "When training on normalized_data_nosens.csv (with sex and race removed), the modelâ€™s accuracy was actually slightly higher than with the full dataset. This suggests that sex and race were not strong independent predictors once other features such as education, occupation, and hours worked were included. Their removal may have reduced noise or collinearity, allowing the model to generalize better. Therefore, we cannot conclude that sex and race have a strong correlation with education or other predictors just based on accuracy results. The increase in accuracy is more likely due to feature redundancy than a lack of real-world correlation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data2060ml)",
   "language": "python",
   "name": "data2060ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
